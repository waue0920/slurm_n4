# H200 叢集壓力測試 I/O 崩潰分析報告 (Job ID: 527)

## 1. 事件概述
在 2026-02-03 進行的 8 節點 x 8 GPU 週期性壓力測試中，作業（Job ID: 527）在啟動後約 **51 秒** 發生 FAILED 異常終止。

## 2. 故障現象
*   **WandB 紀錄中斷**：測試僅完成 4 個迴圈（每個迴圈約 12.5 秒）即停止。
*   **殘留檔案異常**：在 NFS 目錄下發現大量 `test_io_rank_*.tmp` 檔案，且多數檔案大小（~260MB）未達到預期的 512MB。
*   **GPU 負載異常**：GPU 功耗維持在 111W（低負載狀態），顯示運算尚未開始衝壓即因環境因素中斷。

## 3. 根本原因分析 (Root Cause Analysis)
本次崩潰的主因為 **「NFS 併發寫入過載」**：
1.  **過高併發量**：原始腳本讓 64 個 Rank 同時對 NFS 掛載點（Home/Work 目錄）進行每秒數百 MB 的寫入操作。
2.  **網路掛載點瓶頸**：NFS 檔案系統在處理 64 個併發的大型檔案 Metadata 鎖定（File Locking）與數據傳輸時速度劇降，導致 Python 進程進入長時間的 `Uninterruptible Sleep (D state)`。
3.  **Slurm 強制終止**：由於 I/O 阻塞導致心跳或通訊超時，Slurm 偵測到任務掛死（Hung）或資源異常，進而發送 `SIGKILL` 砍掉整個作業。

## 4. WandB 數據表現與效能極限分析
根據崩潰前的 `iter_disk_write_mbps` 圖表觀測：
*   **峰值頻寬**：寫入效能最高落在 **705 MB/s** 左右。
*   **震盪特徵**：在 Step 2 出現明顯下挫（跌至 675 MB/s），顯示伺服器已無法消化 64 個併發進程的寫入請求。
*   **極限結論**：
    *   **頻寬瓶頸**：該 NFS 掛載點的總吞吐量極限約為 **700 MB/s**。
    *   **併發瓶頸**：64 個併發 Rank 是導致系統崩潰的觸發點。當併發數超過伺服器處理能力時，Metadata 延遲會引發全體進程卡死。
    *   **建議配置**：生產環境下，單一掛載點併發建議低於 **8-16 個** 進程，且總頻寬應預留空間，維持在 **500 MB/s** 以下以保證穩定。

## 5. 修復與最佳化措施
為了建立穩定的壓力測試環境，已實施以下修改：

| 測試項目 | 修改前 (Crash 模式) | 修改後 (穩定模式) |
| :--- | :--- | :--- |
| **本地 SSD (/tmp)** | 未測試 | **全量測試** (每節點 8 進程)，增加 50ms 交錯啟動。 |
| **NFS (Home/Work)** | 64 進程併發 | **低併發測試** (每節點僅 1 進程)，全叢集僅 8 個進程操作。 |
| **輸出緩衝** | 預設緩衝 | 強制 `PYTHONUNBUFFERED=1` 確保 Log 即時寫入。 |
| **通訊容忍度** | 預設 | 增加 `NCCL_IB_TIMEOUT=22` 避免 I/O 卡頓引發通訊報錯。 |

## 6. 綜合評價：系統自我保護機制的成功實踐
本次測試觀察到一項關鍵的正向指標：當使用者程式（stress_test.py）因過度衝擊 I/O 極限( 8 nodes x 8 i/o = **64 processes** , 共 **700 MB/s** )而導致系統出現潛在崩潰風險時，叢集防護機制（Slurm/Kernel）能精準地在 51 秒內自動完成介入並中斷程式。

這證明了系統成功啟動自我保護機制：
*   **有效預防基礎設施潰堤**：及時終止異常作業，防止了單一使用者的過激行為導致整台 NFS 伺服器掛死，保障了其他使用者的權益。
*   **資源隔離與故障恢復**：系統能在秒級時間內偵測到 I/O 阻塞並強制回收資源，展現了 H200 叢集在高壓力情境下的彈性與魯棒性。

## 7. 結論與建議
*   **系統防護的強韌性**：實測證明當 64 個併發進程觸發 NFS 頻寬極限（~700 MB/s）並引發潛在風險時，防護機制能在 51 秒內果斷介入，成功保護了基礎設施的完整性。
*   **使用者 I/O 最佳實踐**：
    *   **優先利用本地存儲**：使用者應優先使用 **「節點本地存儲 (/tmp)」** 處理高頻率、高併發的臨時熱數據（如模型權重存取）。
    *   **分層存儲策略**：應明確區分運算時的臨時 I/O 與最終結果的 **「全域共用存儲 (Home/Work)」**。將主要衝壓負載留在本地，僅將最終產出搬移至全域空間，以達成效能與系統穩定性的雙贏。
