#!/bin/bash
#SBATCH --job-name=Yolo9_XXX    ## Job 名稱
#SBATCH --mail-type=ALL            ## 收到通知條件
#SBATCH --mail-user=waue0920@gmail.com
#SBATCH --nodes=4                  ## 索取 x 個節點
#SBATCH --cpus-per-task=32          ## 每個 task 索取 x 顆 CPU
#SBATCH --gres=gpu:8               ## 每個節點索取 x 張 GPU
#SBATCH --account="GOV113038"      ## iService_ID 計畫 ID
#SBATCH --partition=gp4d          ## 使用測試 queue
#SBATCH --output=slurm-yolov9_seg4.log  ## 將標準輸出記錄到 log
#SBATCH --error=slurm-yolov9_seg4.log   ## 將錯誤輸出記錄到同一個 log

# 載入模組
module purge
module load singularity


# singularity command
SIF=/work/waue0920/open_access/yolov9-ngc2111-def-20241115.sif
SINGULARITY="singularity run --nv $SIF"


# defind master
MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1)
export MASTER_ADDR


## twcc 不知為何SLURM_GPUS_ON_NODE 為空
nvidia-smi --list-gpus
gpu_num=$(nvidia-smi -L | wc -l) # $SLURM_GPUS_ON_NODE


echo "==============================="
# 二選一
# * templateV2_yolov9_torchrun_segtrain.sh
# * templateV2_yolov9_torchrun_traindual.sh

cmd="srun --gres=gpu:$gpu_num --mpi=pmix $SINGULARITY bash yolov9_torchrun_segtrain.sh"
#cmd="srun --gres=gpu:$gpu_num --mpi=pmix $SINGULARITY bash yolov9_torchrun_traindual.sh"
echo $cmd
$cmd
