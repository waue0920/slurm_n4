#!/bin/bash
#SBATCH --job-name=Yolo9_x1    ## Job 名稱
#SBATCH --mail-type=ALL            ## 收到通知條件
#SBATCH --mail-user=waue0920@gmail.com
#SBATCH --nodes=1                  ## 索取 x 個節點
#SBATCH --cpus-per-task=32          ## 每個 task 索取 x 顆 CPU
#SBATCH --gres=gpu:8               ## 每個節點索取 x 張 GPU
#SBATCH --account="GOV113038"      ## iService_ID 計畫 ID
#SBATCH --partition=gp2d          ## 使用測試 queue
#SBATCH --output=slurm-yolov9_train1.log  ## 將標準輸出記錄到 log
#SBATCH --error=slurm-yolov9_train1.log   ## 將錯誤輸出記錄到同一個 log

module purge
module load singularity

SIF=/work/waue0920/open_access/yolov9-ngc2111-def-20241115.sif
SINGULARITY="singularity run --nv $SIF"


# defind master
MASTER_ADDR=$(scontrol show hostname $SLURM_NODELIST | head -n 1)
export MASTER_ADDR

nvidia-smi --list-gpus
gpu_num=$(nvidia-smi -L | wc -l)

echo "==============================="
# 執行 `env.sh`，將環境資訊記錄到 log
#srun --mpi=pmix $SINGULARITY bash env.sh
# cmd="srun --gres=gpu:$SLURM_GPUS_ON_NODE --mpi=pmix $SINGULARITY bash env.sh" # twcc 不知為何SLURM_GPUS_ON_NODE 為空
cmd="srun --gres=gpu:$gpu_num --mpi=pmix $SINGULARITY bash yolotrain_torchrun_b128.sh"
echo $cmd
$cmd
